# Batch 1

## benchmarks:

- LongBench: https://arxiv.org/pdf/2308.14508
- Multidimensional Quality Metrics: A Flexible System for Assessing Translation Quality: https://aclanthology.org/2013.tc-1.6.pdf 
- ChatEval: https://arxiv.org/pdf/2308.07201
- Neural Metrics: https://aclanthology.org/2022.wmt-1.2.pdf
- Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation: https://aclanthology.org/2021.tacl-1.87/
- Length-controlled AlpacaEval: https://arxiv.org/html/2404.04475v1
- Metrics Might Be Guilty but References Are Not Innocent: https://aclanthology.org/2023.wmt-1.51/
- xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection: https://arxiv.org/abs/2310.10482
- Human Feedback is not Gold Standard: https://arxiv.org/pdf/2309.16349
- FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models: https://arxiv.org/pdf/2310.20410
- MetricX-23: The Google Submission to the WMT 2023 Metrics Shared Task: https://aclanthology.org/2023.wmt-1.63/
- GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4: https://aclanthology.org/2023.wmt-1.64/
- Holistic Evaluation of Language Models: https://arxiv.org/pdf/2211.09110
- Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models: https://arxiv.org/abs/2402.13887
- MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment
- SacreBLEU: A Call for Clarity in Reporting BLEU Scores: https://aclanthology.org/W18-6319/
- COMET: A Neural Framework for MT Evaluation: https://aclanthology.org/2020.emnlp-main.213/
- BLEURT: Learning Robust Metrics for Text Generation: https://aclanthology.org/2020.acl-main.704/
- Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models: https://arxiv.org/abs/2403.11802
- Style Over Substance: Evaluation Biases for Large Language Models: https://arxiv.org/abs/2307.03025

## RL:
- Reinforcement Learning from LLM Feedback to Counteract Goal Misgeneralization: https://arxiv.org/pdf/2401.07181

## LLMs:

- Beyond English-Centric Multilingual Machine Translation: https://jmlr.org/papers/v22/20-1307.html
- Few-shot: https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
- Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation: https://aclanthology.org/D14-1179.pdf
- SeamlessM4T:  Massively Multilingual & Multimodal Machine Translation: https://arxiv.org/pdf/2308.11596
- Convolutional Sequence to Sequence Learning: http://proceedings.mlr.press/v70/gehring17a.html
- Contrastive Preference Learning: Learning from Human Feedback without RL: https://arxiv.org/abs/2310.13639
- ORPO: Monolithic Preference Optimization without Reference Model: https://arxiv.org/pdf/2403.07691
- LoRA: Low-Rank Adaptation of Large Language Models: https://openreview.net/forum?id=nZeVKeeFYf9
- Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation: https://arxiv.org/pdf/2405.11804
- Chain-of-Dictionary Prompting Elicits Translation in Large Language Models: https://arxiv.org/abs/2305.06575 
- WizardCoder: Empowering Code Large Language Models with Evol-Instruct: https://arxiv.org/pdf/2306.08568
- Direct Preference Optimization: https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf
- Rethinking Document-level Neural Machine Translation: https://aclanthology.org/2022.findings-acl.279/
- Attention is All you Need: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
- Self-Instruct: Aligning Language Models with Self-Generated Instructions: https://aclanthology.org/2023.acl-long.754/
- LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions: https://arxiv.org/abs/2304.14402

- VietAI

## MT:

- On Extrapolation of Long-Text Translation with Large Language Models: file:///home/kaylous/Downloads/Arxiv_LLM_Extrapolation_MT.pdf
- Towards a Literary Machine Translation: The Role of Referential Cohesion: https://aclanthology.org/W12-2503.pdf
- Survey of Low-Resource Machine Translation: https://direct.mit.edu/coli/article/48/3/673/111479/Survey-of-Low-Resource-Machine-Translation
- Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation: https://aclanthology.org/2022.findings-naacl.105.pdf
- Improving Long Context Document-Level Machine Translation: https://aclanthology.org/2023.codi-1.15/
- Meta-Learning for Low-Resource Neural Machine Translation: https://aclanthology.org/D18-1398/
- NON-AUTOREGRESSIVE NEURAL MACHINE TRANSLATION: https://arxiv.org/pdf/1711.02281
- Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs: https://aclanthology.org/2023.wmt-1.3/
- DUTNLP System for the WMT2023 Discourse-Level Literary Translation: https://aclanthology.org/2023.wmt-1.31/

## Multi-agent: 

- Adapting LLM Agents with Universal Communication Feedback: file:///home/kaylous/Downloads/52_Adapting_LLM_Agents_with_Un.pdf
- Self-collaboration code generation via ChatGPT: https://arxiv.org/pdf/2304.07590
- Improving Factuality and Reasoning in Language Models through Multiagent Debate: https://arxiv.org/pdf/2305.14325
- Beyond English-Centric Multilingual Machine Translation: https://jmlr.org/papers/v22/20-1307.html
- Large Language Model based Multi-Agents: A Survey of Progress and Challenges: https://arxiv.org/abs/2402.01680
- MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework: https://arxiv.org/abs/2308.00352
- EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities: https://arxiv.org/abs/2310.10436
- Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate: https://arxiv.org/abs/2305.19118
- Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration: https://arxiv.org/abs/2306.09093
- RoCo: Dialectic Multi-Robot Collaboration with Large Language Models: https://arxiv.org/abs/2307.04738
- WELFARE DIPLOMACY: BENCHMARKING LANGUAGE MODEL COOPERATION: https://arxiv.org/pdf/2310.08901
- Generative Agents: Interactive Simulacra of Human Behavior: https://dl.acm.org/doi/10.1145/3586183.3606763
- ChatDev: Communicative Agents for Software Development: https://arxiv.org/abs/2307.07924
- Mixture Models for Diverse Machine Translation: Tricks of the Trade: http://proceedings.mlr.press/v97/shen19c.html
- Reflexion: language agents with verbal reinforcement learning: https://openreview.net/forum?id=vAElhFcKW6
- Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents: https://arxiv.org/abs/2302.01560
- LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions: https://arxiv.org/abs/2304.14402
- Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf: https://arxiv.org/abs/2309.04658
- MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning: https://arxiv.org/abs/2309.05653
- Building Cooperative Embodied Agents Modularly with Large Language Models: https://arxiv.org/abs/2307.02485
- BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models: https://arxiv.org/abs/2306.10968
- Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: https://arxiv.org/abs/2306.05685
- Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models: https://arxiv.org/abs/2406.06910
- Emergent Translation in Multi-Agent Communication: https://arxiv.org/abs/1710.06922
- SiLLM: Large Language Models for Simultaneous Machine Translation: https://arxiv.org/pdf/2402.13036

## Others:

- The Last Frontier of Machine Translation: https://www.theatlantic.com/technology/archive/2024/01/literary-translation-artificial-intelligence/677038/
- Findings of the 2023 Conference on Machine Translation (WMT23): LLMs Are Here But Not Quite There Yet : https://aclanthology.org/2023.wmt-1.1.pdf
- Intelligent agents: theory and practice: https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/intelligent-agents-theory-and-practice/CF2A6AAEEA1DBD486EF019F6217F1597
- Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models: https://arxiv.org/abs/2309.01219

## Datasets:

- Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature: https://aclanthology.org/2022.emnlp-main.672.pdf

# Batch 2:

## benchmarks:

- Multidimensional Quality Metrics: A Flexible System for Assessing Translation Quality: https://aclanthology.org/2013.tc-1.6.pdf : assess dataset quality

- ChatEval: https://arxiv.org/pdf/2308.07201: Multi-agent judge

- Neural Metrics: https://aclanthology.org/2022.wmt-1.2.pdf

- Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation: https://aclanthology.org/2021.tacl-1.87/
- Length-controlled AlpacaEval: https://arxiv.org/html/2404.04475v1
- Metrics Might Be Guilty but References Are Not Innocent: https://aclanthology.org/2023.wmt-1.51/
- xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection: https://arxiv.org/abs/2310.10482
- Human Feedback is not Gold Standard: https://arxiv.org/pdf/2309.16349
- FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models: https://arxiv.org/pdf/2310.20410
- MetricX-23: The Google Submission to the WMT 2023 Metrics Shared Task: https://aclanthology.org/2023.wmt-1.63/
- GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4: https://aclanthology.org/2023.wmt-1.64/
- Holistic Evaluation of Language Models: https://arxiv.org/pdf/2211.09110
- Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models: https://arxiv.org/abs/2402.13887
- MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment
- SacreBLEU: A Call for Clarity in Reporting BLEU Scores: https://aclanthology.org/W18-6319/
- COMET: A Neural Framework for MT Evaluation: https://aclanthology.org/2020.emnlp-main.213/
- BLEURT: Learning Robust Metrics for Text Generation: https://aclanthology.org/2020.acl-main.704/
- Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models: https://arxiv.org/abs/2403.11802
- Style Over Substance: Evaluation Biases for Large Language Models: https://arxiv.org/abs/2307.03025
- Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: https://arxiv.org/abs/2306.05685

## Multi-agent: 

### Code:
- Self-collaboration code generation via ChatGPT: https://arxiv.org/pdf/2304.07590
- MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework: https://arxiv.org/abs/2308.00352
- ChatDev: Communicative Agents for Software Development: https://arxiv.org/abs/2307.07924
- TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation: https://arxiv.org/abs/2409.19894

### Game:
- WELFARE DIPLOMACY: BENCHMARKING LANGUAGE MODEL COOPERATION: https://arxiv.org/pdf/2310.08901
- Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf: https://arxiv.org/abs/2309.04658

### Reasoning:
- Adapting LLM Agents with Universal Communication Feedback: file:///home/kaylous/Downloads/52_Adapting_LLM_Agents_with_Un.pdf
- Improving Factuality and Reasoning in Language Models through Multiagent Debate: https://arxiv.org/pdf/2305.14325
- MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning: https://arxiv.org/abs/2309.05653
- Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate: https://arxiv.org/abs/2305.19118

- Large Language Model based Multi-Agents: A Survey of Progress and Challenges: https://arxiv.org/abs/2402.01680

### Single agent:
- EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities: https://arxiv.org/abs/2310.10436
- Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration: https://arxiv.org/abs/2306.09093
- Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents: https://arxiv.org/abs/2302.01560

### RL:
- Multi-Agent Reinforcement Learning with Selective State-Space Models: https://arxiv.org/abs/2410.19382
- Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning: https://arxiv.org/abs/2410.17373
- Reflexion: language agents with verbal reinforcement learning: https://openreview.net/forum?id=vAElhFcKW6
- SELF-REFINE: Iterative Refinement with Self-Feedback: https://arxiv.org/pdf/2303.17651
- Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback: https://arxiv.org/pdf/2302.12813

### Other:
- RoCo: Dialectic Multi-Robot Collaboration with Large Language Models: https://arxiv.org/abs/2307.04738
- Generative Agents: Interactive Simulacra of Human Behavior: https://dl.acm.org/doi/10.1145/3586183.3606763
- Building Cooperative Embodied Agents Modularly with Large Language Models: https://arxiv.org/abs/2307.02485

### MT-related:
- Mixture Models for Diverse Machine Translation: Tricks of the Trade: http://proceedings.mlr.press/v97/shen19c.html
- SiLLM: Large Language Models for Simultaneous Machine Translation: https://arxiv.org/pdf/2402.13036
- Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models: https://arxiv.org/abs/2406.06910
- Emergent Translation in Multi-Agent Communication: https://arxiv.org/abs/1710.06922
- BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models: https://arxiv.org/abs/2306.10968

- Zero-Resource Neural Machine Translation with Multi-Agent Communication Game: https://aaai.org/papers/11976-zero-resource-neural-machine-translation-with-multi-agent-communication-game/
- Multi-agent Learning for Neural Machine Translation: https://aclanthology.org/D19-1079/
- Multi-Agent Mutual Learning at Sentence-Level and Token-Level for Neural Machine Translation: https://aclanthology.org/2020.findings-emnlp.155/
