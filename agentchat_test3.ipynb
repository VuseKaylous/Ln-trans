{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5942e840-ebb4-4027-b78e-57af78b5c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import numpy\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "from openai import BadRequestError\n",
    "\n",
    "import autogen\n",
    "from autogen import UserProxyAgent, AssistantAgent, ConversableAgent\n",
    "from autogen import config_list_from_json\n",
    "from autogen.agentchat import Agent\n",
    "from autogen.agentchat.contrib.agent_optimizer import AgentOptimizer\n",
    "from autogen.agentchat.contrib.math_user_proxy_agent import MathUserProxyAgent\n",
    "from autogen.code_utils import extract_code\n",
    "from autogen.math_utils import get_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a52f8-15c9-40f4-a5de-af6b34889e47",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2215658f-78f8-4a7d-912a-6db8e14fdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(url):\n",
    "    file = open(url, \"r\")\n",
    "    data = file.read().split('\\n')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1402f6-b7e8-4d1f-9496-35cf6fceb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoMT_dev_en = read_file(\"data/PhoMT/detokenization/dev/dev.en\")\n",
    "phoMT_dev_vi = read_file(\"data/PhoMT/detokenization/dev/dev.vi\")\n",
    "phoMT_test_en = read_file(\"data/PhoMT/detokenization/test/test.en\")\n",
    "phoMT_test_vi = read_file(\"data/PhoMT/detokenization/test/test.vi\")\n",
    "phoMT_train_en = read_file(\"data/PhoMT/detokenization/train/train.en\")\n",
    "phoMT_train_vi = read_file(\"data/PhoMT/detokenization/train/train.vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9baa46-f350-48e2-b961-8c2d90713847",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoMT_dev_en[0] = phoMT_dev_en[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49c978d-07e0-4e71-bf95-c1ba9696ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoMT_dev_envi = [];\n",
    "for index in range(len(phoMT_dev_en)):\n",
    "    phoMT_dev_envi.append({\"question\":phoMT_dev_en[index], \"answer\": phoMT_dev_vi[index]})\n",
    "phoMT_test_envi = [];\n",
    "for index in range(len(phoMT_test_en)):\n",
    "    phoMT_test_envi.append({\"question\": phoMT_test_en[index], \"answer\": phoMT_test_vi[index]})\n",
    "phoMT_train_envi = [];\n",
    "for index in range(len(phoMT_train_en)):\n",
    "    phoMT_train_envi.append({\"question\": phoMT_train_en[index], \"answer\": phoMT_train_vi[index]})\n",
    "# phoMT_dev_envi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d088d4c-ae88-4867-a876-17485e8a8dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hurricane Dorian, one of the most powerful storms ever recorded in the Atlantic Ocean, made landfall as a Category 5 storm on Great Abaco Island in the northern Bahamas on Sunday morning, September 1, 2019.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoMT_dev_envi[0][\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89700002-0754-44ca-a3bb-89820799ed1e",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac70eb-557b-446c-99a5-777a9f9eb360",
   "metadata": {},
   "source": [
    "### COMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49bee0e-f1bd-44e2-b28a-40b8ebb4fc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaylous/workspace/ics/llms/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/home/kaylous/workspace/ics/llms/.venv/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Load the model checkpoint:\n",
    "model = load_from_checkpoint('./XCOMET-XL/checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2acb6a03-42e7-474f-a251-6aadf4436c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_comet(src, ans, res):\n",
    "    data = [\n",
    "        {\n",
    "            \"src\": src,\n",
    "            \"mt\" : res,\n",
    "            \"ref\": ans\n",
    "        }\n",
    "    ]\n",
    "    return model.predict(data, batch_size=8, gpus=1).system_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc029d67-6409-4893-b3c8-65c07861c548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07f93b19-da94-41a0-a239-132589729474",
   "metadata": {},
   "source": [
    "### SacreBleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1946652-7982-4204-88c3-a435d11c613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "bleu = BLEU()\n",
    "\n",
    "def get_score(src, ans, res):\n",
    "     return bleu.corpus_score([ans], [[res]]).score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b8b20-9848-4983-9cdf-1762b1d6389e",
   "metadata": {},
   "source": [
    "## Agent init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dac48d-5477-4b07-bb3e-57dd139b9f27",
   "metadata": {},
   "source": [
    "### Agents declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d597e883-98d0-4ca7-b3df-b7cd140cb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"llama3\",\n",
    "            \"base_url\": \"http://localhost:11434/v1\",\n",
    "            \"api_key\": \"ollama\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     name=\"Userproxyagent\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     code_execution_config={\"work_dir\": \"_output\", \"use_docker\": False},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b879a2-0771-4521-85a0-faa076ae3b20",
   "metadata": {},
   "source": [
    "### Custom UserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bad383b-7b8e-4483-bd35-7a29ba6ea47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeProxyAgent(UserProxyAgent):\n",
    "    MAX_CONSECUTIVE_AUTO_REPLY = 10\n",
    "    DEFAULT_REPLY_TEMPLATE = \"Generate a response more closely resembling the style, detail, and tone of the provided answer. Focus on specifying key elements to capture the nuances of this answer effectively. The answer: \"\n",
    "    PROMPTS = \"\"\"Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
    "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
    "    The text:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: Optional[str] = \"JudgeChatAgent\",\n",
    "        human_input_mode: Literal[\"ALWAYS\", \"NEVER\", \"TERMINATE\"] = \"NEVER\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            human_input_mode=human_input_mode,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.register_reply(\n",
    "            trigger=ConversableAgent, reply_func=JudgeProxyAgent._check_final_result, position=0\n",
    "        )\n",
    "        self.max_function_call_trial = 3\n",
    "        self.query = None\n",
    "        self._answer = None\n",
    "        self.is_correct = None\n",
    "        self.previous = None\n",
    "        # self.history = []\n",
    "\n",
    "    def initiate_chat(\n",
    "        self,\n",
    "        recipient,\n",
    "        history: List[Dict] = [],\n",
    "        answer=None,\n",
    "        previous=None,\n",
    "        silent: Optional[bool] = False,\n",
    "        max_turns = 3,\n",
    "        **context,\n",
    "    ):\n",
    "        self.query = context[\"message\"]\n",
    "        self._answer = answer\n",
    "        self.previous = previous\n",
    "        \n",
    "        self.is_correct = None\n",
    "        self.max_function_call_trial = max_turns\n",
    "        recipient.history = history\n",
    "        print(\"Recipient.history: \" + str(len(recipient.history)))\n",
    "        \n",
    "        self._prepare_chat(recipient, True)\n",
    "        \n",
    "        # for msg in history:\n",
    "        #     self._append_oai_message(\n",
    "        #         message=msg['content'],\n",
    "        #         role=msg['role'],\n",
    "        #         conversation_id = recipient if msg['role'] == \"user\" else self\n",
    "        #     )\n",
    "        \n",
    "        error_message = None\n",
    "        try:\n",
    "            prompt = self.PROMPTS + context['message']\n",
    "            self.send(prompt, recipient, silent=silent)\n",
    "        except BadRequestError as e:\n",
    "            error_message = str(e)\n",
    "            self.is_correct = 0\n",
    "            print(\"error information: {}\".format(error_message))\n",
    "\n",
    "        recipient.reset()\n",
    "        self.is_correct = copy.deepcopy(self.is_correct)\n",
    "        result = self.is_correct\n",
    "        self._reset()\n",
    "        return result\n",
    "\n",
    "    def receive(\n",
    "        self,\n",
    "        message: Union[Dict, str],\n",
    "        sender: Agent,\n",
    "        request_reply: Optional[bool] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "    ):\n",
    "        self._process_received_message(message, sender, silent)\n",
    "        if request_reply is False or request_reply is None and self.reply_at_receive[sender] is False:\n",
    "            return\n",
    "\n",
    "        self.is_correct = self.chat_messages[sender][-1].get(\"content\")\n",
    "        if self._answer is not None:\n",
    "            if (get_score(self.query, self.is_correct, self._answer) >= 90):\n",
    "                return\n",
    "\n",
    "        self.max_function_call_trial = self.max_function_call_trial - 1\n",
    "        if (self.max_function_call_trial <= 0):\n",
    "            self.max_function_call_trial = 0\n",
    "            return\n",
    "\n",
    "        # reply = f\"Using the original sentence: {self.query}, provide guidance to improve the quality of {self.is_correct}. Focus on enhancing accuracy, tone, fluency, and contextual appropriateness.\"\n",
    "        reply = f\"Translate the sentence: {self.query} into Vietnamese, ensuring that when the translation is appended to the previous translated paragraph: {self.previous}, the entire paragraph remains coherent, meaningful, and contextually appropriate. Respond with only the translated sentence, without any additional commentary or explanation.\"\n",
    "        if self._answer is not None:\n",
    "            reply = f'Analyze the original sentence: {self.query}, the expected Vietnamese translation: {self._answer}, and the generated translation: {self.is_correct}. Identify the differences between {self.is_correct} and {self._answer}, and provide guidance to improve the translation so it aligns more closely with {self._answer}. Focus on preserving meaning, tone, style, and naturalness in Vietnamese while addressing any discrepancies.'\n",
    "        # if self._answer is not None:\n",
    "        #     reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)\n",
    "        self.send(reply, sender, silent=silent)\n",
    "\n",
    "    def _check_final_result(\n",
    "        self,\n",
    "        messages: Optional[List[Dict]] = None,\n",
    "        sender: Optional[Agent] = None,\n",
    "        config: Optional[Any] = None,\n",
    "    ):\n",
    "        messages = messages[-1]\n",
    "        if isinstance(messages, dict):\n",
    "            messages = messages.get(\"content\")\n",
    "            if messages is None:\n",
    "                return False, None\n",
    "            if (messages.find(\"\\n\") >= 0):\n",
    "                print(\"Response longer than expected?\\n\" + messages)\n",
    "                # messages = messages.split(\"\\n\")[0]\n",
    "\n",
    "        self.is_correct = messages\n",
    "        temp_score = get_score(self.query, messages, self._answer)\n",
    "        print(\"Score: \" + str(temp_score))\n",
    "        if (temp_score >= 90):\n",
    "            return True, \"The result is passable. Please reply me with the same answer as before.\"\n",
    "        return False, None\n",
    "\n",
    "    def _reset(self):\n",
    "        # super()._reset()\n",
    "        self.max_function_call_trial = 0\n",
    "        self.is_correct = None\n",
    "        self.query = None\n",
    "        self._answer = None\n",
    "        self.previous = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7025f1-fe18-49d5-9c42-6de8dfdebbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptAssistant(AssistantAgent):\n",
    "    PROMPTS = \"\"\"Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
    "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
    "    The text:\n",
    "    \"\"\"\n",
    "    TRIMMING_PROMPT = \"Respond with only the improved translation, without any additional explanations or commentary.\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        after: Agent,\n",
    "        before: Agent = None,\n",
    "        name: Optional[str] = \"PromptAssistantAgent\",\n",
    "        human_input_mode: Literal[\"ALWAYS\", \"NEVER\", \"TERMINATE\"] = \"NEVER\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            human_input_mode=human_input_mode,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.before = before\n",
    "        self.after = after\n",
    "        self.history = []\n",
    "        self.is_start = False\n",
    "        self.last_msg = \"NaH\"\n",
    "\n",
    "    def initiate_chat(\n",
    "        self,\n",
    "        recipient,\n",
    "        history: List[Dict] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "        max_turns = 3,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.is_start = True\n",
    "        self._prepare_chat(recipient, True)\n",
    "        for msg in history:\n",
    "            self._append_oai_message(\n",
    "                message=msg['content'],\n",
    "                role=msg['role'],\n",
    "                conversation_id = recipient\n",
    "            )\n",
    "        print(len(self.chat_messages_for_summary(recipient)))\n",
    "        error_message = None\n",
    "        try:\n",
    "            prompt = self.PROMPTS + kwargs['message']\n",
    "            self.send(prompt, recipient, silent=silent)\n",
    "        except BadRequestError as e:\n",
    "            error_message = str(e)\n",
    "            print(\"error information: {}\".format(error_message))\n",
    "        recipient.reset()\n",
    "        self.is_start = False\n",
    "        print(self.chat_messages_for_summary(recipient))\n",
    "        return self.chat_messages_for_summary(recipient)[-1]['content']\n",
    "\n",
    "    def receive(\n",
    "        self,\n",
    "        message: Union[Dict, str],\n",
    "        sender: Agent,\n",
    "        request_reply: Optional[bool] = None,\n",
    "        silent: Optional[bool] = False,\n",
    "    ):\n",
    "        self._process_received_message(message, sender, silent)\n",
    "        if request_reply is False or request_reply is None and self.reply_at_receive[sender] is False:\n",
    "            return\n",
    "        if self.is_start:\n",
    "            # reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)\n",
    "            # if reply is not None:\n",
    "            #     self.send(reply, sender, silent=silent)\n",
    "            return\n",
    "        if (sender == self.before):\n",
    "            reply = sender.chat_messages_for_summary(self)[0]['content']\n",
    "            if len(sender.chat_messages_for_summary(self)) > 1:\n",
    "                state, res = self.generate_oai_reply(messages=self.chat_messages[sender], sender=sender)\n",
    "                if state:\n",
    "                    reply = res\n",
    "                # reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)\n",
    "            else:\n",
    "                print(\"Self.history: \" + str(len(self.history)))\n",
    "                for msg in self.history:\n",
    "                    self._append_oai_message(\n",
    "                        message=msg['content'],\n",
    "                        role=msg['role'],\n",
    "                        conversation_id = self.after\n",
    "                    )\n",
    "            if reply is not None:\n",
    "                reply = reply + \"\\n\\n\" + self.TRIMMING_PROMPT\n",
    "                self.last_msg = reply\n",
    "                print(self.last_msg)\n",
    "                self.send(reply, self.after, silent=silent, request_reply=True)\n",
    "                if (self._oai_messages[self.after][-1]['content'].find('Note') != -1):\n",
    "                    temp = self._oai_messages[self.after][-1]['content']\n",
    "                    self._oai_messages[self.after][-1]['content'] = temp[:temp.find('Note')].strip()\n",
    "                # print(self.chat_messages_for_summary(self.after))\n",
    "                self.history = self.chat_messages_for_summary(self.after)\n",
    "                self.send(self.chat_messages_for_summary(self.after)[-1]['content'], self.before, silent=silent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09036ef3-0856-4667-bbfc-7033c6895bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = autogen.AssistantAgent(\n",
    "    name=\"LLM\",\n",
    "    system_message=\"You are a helpful assistant\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "Judge = JudgeProxyAgent(\n",
    "    name=\"Judge\",\n",
    "    system_message=\"You are an advisor\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "PromptGenerator = PromptAssistant(\n",
    "    before=Judge,\n",
    "    after=LLM,\n",
    "    name=\"PromptGenerator\",\n",
    "    system_message=\"You are a prompt engineer\",\n",
    "    human_input_mode = \"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28350bf-09e8-44ea-b12f-ecd6fc18e04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result = Judge.initiate_chat(\n",
    "#     recipient = PromptGenerator,\n",
    "#     max_turns = 2,\n",
    "#     message = phoMT_dev_envi[4][\"question\"],\n",
    "#     # answer = phoMT_dev_envi[4][\"answer\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c705639d-5684-4f7c-99d6-b330facac746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PromptGenerator.last_msg\n",
    "# PromptGenerator.history\n",
    "    # print(Judge.chat_messages(PromptGenerator)[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "105600c3-6a93-441c-95c1-fe49d0360022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptGenerator.chat_messages_for_summary(LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f2ea25f-3465-4d94-8850-74f09934c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f213e8b-c2cc-4539-a72e-2fd993d542c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge_result = \"Tới thời điểm hiện nay, không có tin tức về người bị thương trong 46 Nhà xuất bản thuộc hai giáo hội trên đảo Abaco lớn.\"\n",
    "# get_score(phoMT_dev_envi[4][\"question\"], result, phoMT_dev_envi[4][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6e064c9-a0c9-477f-b38f-c5f33ddb00bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result = PromptGenerator.initiate_chat(\n",
    "#     recipient = LLM,\n",
    "#     max_turns = 2,\n",
    "#     message = phoMT_dev_envi[4][\"question\"],\n",
    "#     history = PromptGenerator.history,\n",
    "#     clear_history = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14fd2784-398b-44db-8267-8651d9b32299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_translate(message, answer, turns = 1, history = None):\n",
    "    if history is None:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            answer = answer,\n",
    "        )\n",
    "    else:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            answer = answer,\n",
    "            history = history\n",
    "        )\n",
    "    return [get_score(message, str(result), answer) / 100.0, get_score_comet(message, str(result), answer)]\n",
    "\n",
    "def score_translate_test(message, answer, turns = 1, history = None, context = None):\n",
    "    if history is None:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            previous = context\n",
    "        )\n",
    "    else:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            history = history,\n",
    "            previous = context\n",
    "        )\n",
    "    # result = PromptGenerator.initiate_chat(\n",
    "    #     recipient = LLM,\n",
    "    #     max_turns = turns + 1,\n",
    "    #     message = message,\n",
    "    #     history = history,\n",
    "    #     clear_history = False\n",
    "    # )\n",
    "    # return result\n",
    "    return [get_score(message, str(result), answer) / 100.0, get_score_comet(message, str(result), answer)]\n",
    "\n",
    "def score_translate_comet(message, answer, turns = 1, history = None):\n",
    "    if history is None:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            answer = answer,\n",
    "        )\n",
    "    else:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            answer = answer,\n",
    "            history = history\n",
    "        )\n",
    "    return get_score_comet(message, str(result), answer)\n",
    "\n",
    "def score_translate_comet_test(message, answer, turns = 1, history = None, context = None):\n",
    "    if history is None:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            previous = context\n",
    "        )\n",
    "    else:\n",
    "        result = Judge.initiate_chat(\n",
    "            recipient = PromptGenerator,\n",
    "            max_turns = turns + 1,\n",
    "            message = message,\n",
    "            history = history,\n",
    "            previous = context\n",
    "        )\n",
    "    return get_score_comet(message, str(result), answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80678b6e-ea4a-42c6-a437-98752f941b40",
   "metadata": {},
   "source": [
    "## Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8f9df11-4d61-4964-8313-070c21110469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19152"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phoMT_test_envi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ec228ec-31db-4439-8cba-1ad5240f70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 1\n",
    "test_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4108463f-4296-420b-8d98-3c843f1d7172",
   "metadata": {},
   "source": [
    "### Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2eb9a24-67e5-4ece-a2d6-eca914122a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipient.history: 0\n",
      "\u001b[33mJudge\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Self.history: 0\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\u001b[33mPromptGenerator\u001b[0m (to LLM):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:19:50] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLLM\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Cơn bão mạnh đã qua các khu vực miền nam và trung tâm Hoa Kỳ vào ngày 11-12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPromptGenerator\u001b[0m (to Judge):\n",
      "\n",
      "Cơn bão mạnh đã qua các khu vực miền nam và trung tâm Hoa Kỳ vào ngày 11-12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# score_dev = []\n",
    "# for i in range(10):\n",
    "#     x = phoMT_dev_envi[i]\n",
    "#     score_dev.append(score_translate(x['question'], x['answer'], 0))\n",
    "score_test = []\n",
    "for i in range(test_num):\n",
    "    x = phoMT_test_envi[train_num + i]\n",
    "    score_test.append(score_translate(\n",
    "        message= x['question'],\n",
    "        answer= x['answer'],\n",
    "        turns= 0\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39132230-6015-4137-96cf-67d744e14469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round(numpy.average(score_test), 2)\n",
    "round(numpy.average([x[0] for x in score_test]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c6d58cd-ab0d-46b8-b777-bec2b5fed2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(numpy.average([x[1] for x in score_test]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed876041-caed-4088-ba21-892d1daf7e01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.07466558730061357, 0.9703085422515869]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e93be7d-49c7-448c-b4e4-544606f51e53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test = []\n",
    "# for i in range(test_num):\n",
    "#     x = phoMT_test_envi[train_num + i]\n",
    "#     score_test.append(score_translate_comet(\n",
    "#         message= x['question'],\n",
    "#         answer= x['answer'],\n",
    "#         turns= 0\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca29b962-7284-4700-a57c-92529669695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round(numpy.average(score_test), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33d0786a-a4b0-448f-b3bb-2dcbe8f9f977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6803da8d-e0e3-4c70-bc08-316f4085131c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2978000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phoMT_train_envi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c54875-3daf-4788-a0b5-77232b6858b9",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f811dd24-9367-4061-a3f9-67460c72f531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipient.history: 0\n",
      "\u001b[33mJudge\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Brother Albert Barnett and his wife, Sister Susan Barnett, from the West Congregation in Tuscaloosa, Alabama\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Self.history: 0\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Brother Albert Barnett and his wife, Sister Susan Barnett, from the West Congregation in Tuscaloosa, Alabama\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\u001b[33mPromptGenerator\u001b[0m (to LLM):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Brother Albert Barnett and his wife, Sister Susan Barnett, from the West Congregation in Tuscaloosa, Alabama\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:09] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLLM\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Anh em Albert Barnett và vợ anh, Chị Susan Barnett, từ Lãnh hội Tây tại Tuscaloosa, Alabama.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPromptGenerator\u001b[0m (to Judge):\n",
      "\n",
      "Anh em Albert Barnett và vợ anh, Chị Susan Barnett, từ Lãnh hội Tây tại Tuscaloosa, Alabama.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mJudge\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Analyze the original sentence: Brother Albert Barnett and his wife, Sister Susan Barnett, from the West Congregation in Tuscaloosa, Alabama, the expected Vietnamese translation: Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama, and the generated translation: Anh em Albert Barnett và vợ anh, Chị Susan Barnett, từ Lãnh hội Tây tại Tuscaloosa, Alabama.. Identify the differences between Anh em Albert Barnett và vợ anh, Chị Susan Barnett, từ Lãnh hội Tây tại Tuscaloosa, Alabama. and Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama, and provide guidance to improve the translation so it aligns more closely with Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama. Focus on preserving meaning, tone, style, and naturalness in Vietnamese while addressing any discrepancies.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:16] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of the differences:\n",
      "\n",
      "1. Anh em Albert Barnett và vợ anh, Chị Susan Barnett: The generated translation uses \"anh em\" (brothers) to translate \"Brother\", whereas a more accurate translation would use \"Anh\" (older brother or Mr.) for a masculine title and \"Chị\" (older sister or Mrs.) for a feminine title. This mismatch affects the tone and style of the translation.\n",
      "\n",
      "2. từ Lãnh hội Tây: The generated translation uses the phrase \"từ Lãnh hội Tây\", which doesn't convey the correct meaning. The phrase should be \" thuộc hội thánh West\" to correctly translate \"from the West Congregation\".\n",
      "\n",
      "3. tại Tuscaloosa, Alabama: This part is accurately translated, conveying the location information.\n",
      "\n",
      "To improve the translation and align it with Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama:\n",
      "\n",
      "* Use the correct translations for masculine (\"Anh\") and feminine titles (\"Chị\") to maintain tone and style consistency.\n",
      "* Replace \"từ Lãnh hội Tây\" with \"thuộc hội thánh West\" to convey the correct meaning.\n",
      "* Retain the accurate translation of location information (\"tại Tuscaloosa, Alabama\").\n",
      "\n",
      "The revised translation should read:\n",
      "\n",
      "Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\u001b[33mPromptGenerator\u001b[0m (to LLM):\n",
      "\n",
      "Analysis of the differences:\n",
      "\n",
      "1. Anh em Albert Barnett và vợ anh, Chị Susan Barnett: The generated translation uses \"anh em\" (brothers) to translate \"Brother\", whereas a more accurate translation would use \"Anh\" (older brother or Mr.) for a masculine title and \"Chị\" (older sister or Mrs.) for a feminine title. This mismatch affects the tone and style of the translation.\n",
      "\n",
      "2. từ Lãnh hội Tây: The generated translation uses the phrase \"từ Lãnh hội Tây\", which doesn't convey the correct meaning. The phrase should be \" thuộc hội thánh West\" to correctly translate \"from the West Congregation\".\n",
      "\n",
      "3. tại Tuscaloosa, Alabama: This part is accurately translated, conveying the location information.\n",
      "\n",
      "To improve the translation and align it with Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama:\n",
      "\n",
      "* Use the correct translations for masculine (\"Anh\") and feminine titles (\"Chị\") to maintain tone and style consistency.\n",
      "* Replace \"từ Lãnh hội Tây\" with \"thuộc hội thánh West\" to convey the correct meaning.\n",
      "* Retain the accurate translation of location information (\"tại Tuscaloosa, Alabama\").\n",
      "\n",
      "The revised translation should read:\n",
      "\n",
      "Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:17] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLLM\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPromptGenerator\u001b[0m (to Judge):\n",
      "\n",
      "Anh Albert Barnett và chị Susan Barnett, thuộc hội thánh West ở Tuscaloosa, Alabama.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1\n",
    "turns = 1\n",
    "# optimizer_model = \"gpt-4-1106-preview\"\n",
    "# optimizer = AgentOptimizer(max_actions_per_step=3, llm_config=llm_config, optimizer_model=\"llama3\")\n",
    "history_recorder = []\n",
    "last_msg_s = []\n",
    "for index in range(train_num):\n",
    "    query = phoMT_test_envi[index]\n",
    "    # is_correct = user_proxy.initiate_chat(assistant, answer=query[\"answer\"], problem=query[\"question\"])\n",
    "    result = score_translate(query['question'], query['answer'], turns)\n",
    "    history = PromptGenerator.history\n",
    "    print(f\"Test: {index}\")\n",
    "    history_recorder.extend(history)\n",
    "    last_msg_s.append(PromptGenerator.last_msg)\n",
    "    print(len(history_recorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "064560b6-162c-488e-be25-5e41e31cbd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history_recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc36fba8-8833-4461-a946-adab5aae7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the history to .json file for future usage\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(history_recorder, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a27ff3-ae5a-4c2c-ab6d-f6f18e6ef145",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "791d418b-f7e2-48dc-b93c-251f571b8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the history from data.json file\n",
    "with open('data.json', 'r') as file:\n",
    "    history_recorder = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d701541-1c04-41ff-b2e1-5a885ced9a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0:\n",
      "Recipient.history: 4\n",
      "\u001b[33mJudge\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Self.history: 4\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\u001b[33mPromptGenerator\u001b[0m (to LLM):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:23] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLLM\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Cơn bão nghiêm trọng đã qua các khu vực miền nam và trung tâm Hoa Kỳ vào ngày 11-12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPromptGenerator\u001b[0m (to Judge):\n",
      "\n",
      "Cơn bão nghiêm trọng đã qua các khu vực miền nam và trung tâm Hoa Kỳ vào ngày 11-12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "score_test = []\n",
    "answers = []\n",
    "for i in range(test_num):\n",
    "    x = phoMT_test_envi[train_num+i]\n",
    "    print(\"Test \" + str(i) + \":\")\n",
    "    context = \" \".join(answers)\n",
    "    # if (len(answers)>5) context = answers[-5:].join(\" \")\n",
    "    score_test.append(score_translate_test(\n",
    "        message = x['question'],\n",
    "        answer = x['answer'],\n",
    "        turns = 0,\n",
    "        history = history_recorder,\n",
    "        context = context\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4c591c2-66dd-41be-94ce-fd32e374bec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round(numpy.average(score_test), 2)\n",
    "round(numpy.average([x[0] for x in score_test]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5274547d-de04-476b-888a-a439d227783a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(numpy.average([x[1] for x in score_test]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cf9a2af-62a5-4d5e-a57d-32251ebcaa6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3dfecf8-3023-4ee4-b107-ed79965734f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0:\n",
      "Recipient.history: 4\n",
      "\u001b[33mJudge\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Self.history: 4\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\u001b[33mPromptGenerator\u001b[0m (to LLM):\n",
      "\n",
      "Translate a sentence from English to Vietnamese. Produce an accurate, context-sensitive translation that maintains the tone and meaning of the original sentence. Ensure that the output sounds natural for native Vietnamese speakers.\n",
      "    Respond only with the requested output. Do not include any explanations, introductions, follow-up remarks, or additional feedback. Provide exactly and only what is specified in the task.\n",
      "    The text:\n",
      "    Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:29] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLLM\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Cơn bão mạnh đã qua các khu vực miền nam và trung tâm Hoa Kỳ vào ngày 11-12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPromptGenerator\u001b[0m (to Judge):\n",
      "\n",
      "Cơn bão mạnh đã qua các khu vực miền nam và trung tâm Hoa Kỳ vào ngày 11-12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mJudge\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Translate the sentence: Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12, 2020. into Vietnamese, ensuring that when the translation is appended to the previous translated paragraph: , the entire paragraph remains coherent, meaningful, and contextually appropriate. Respond with only the translated sentence, without any additional commentary or explanation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:30] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cơn bão nghiêm trọng đã qua các khu vực phía nam và trung tâm Hoa Kỳ vào ngày 11 và 12 tháng một năm 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\u001b[33mPromptGenerator\u001b[0m (to LLM):\n",
      "\n",
      "Cơn bão nghiêm trọng đã qua các khu vực phía nam và trung tâm Hoa Kỳ vào ngày 11 và 12 tháng một năm 2020.\n",
      "\n",
      "Respond with only the improved translation, without any additional explanations or commentary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-10 09:20:44] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLLM\u001b[0m (to PromptGenerator):\n",
      "\n",
      "Cơn bão nghiêm trọng đã qua các khu vực phía nam và trung tâm Hoa Kỳ vào ngày 11 và 12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPromptGenerator\u001b[0m (to Judge):\n",
      "\n",
      "Cơn bão nghiêm trọng đã qua các khu vực phía nam và trung tâm Hoa Kỳ vào ngày 11 và 12 tháng một năm 2020.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "score_test = []\n",
    "answers = []\n",
    "for i in range(test_num):\n",
    "    x = phoMT_test_envi[train_num+i]\n",
    "    print(\"Test \" + str(i) + \":\")\n",
    "    context = \" \".join(answers)\n",
    "    # if (len(answers)>5) context = answers[-5:].join(\" \")\n",
    "    score_test.append(score_translate_test(\n",
    "        message = x['question'],\n",
    "        answer = x['answer'],\n",
    "        turns = 1,\n",
    "        history = history_recorder,\n",
    "        context = context\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02baea81-768b-4742-b1a2-a8fefe013c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round(numpy.average(score_test), 2)\n",
    "round(numpy.average([x[0] for x in score_test]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b592195e-4a9e-49a2-88a7-15b6bf933f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(numpy.average([x[1] for x in score_test]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a65b1dd-6672-4d68-ad1a-db354457d92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d7625de-da56-4bd6-9533-27bbfeb29e07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test = []\n",
    "# answers = []\n",
    "# for i in range(test_num):\n",
    "#     x = phoMT_test_envi[train_num+i]\n",
    "#     print(\"Test \" + str(i) + \":\")\n",
    "#     context = \" \".join(answers)\n",
    "#     # if (len(answers)>5) context = answers[-5:].join(\" \")\n",
    "#     score_test.append(score_translate_comet_test(\n",
    "#         message = x['question'],\n",
    "#         answer = x['answer'],\n",
    "#         turns = 0,\n",
    "#         history = history_recorder,\n",
    "#         context = context\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34b8c397-cd7f-462b-827d-f88e5ed29f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round(numpy.average(score_test), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71fed9b0-d4d7-44c0-9c5b-314baeb599bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b670fddb-43a5-4b07-8f9e-65c3a9712034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score_test = []\n",
    "# answers = []\n",
    "# for i in range(test_num):\n",
    "#     x = phoMT_test_envi[train_num+i]\n",
    "#     print(\"Test \" + str(i) + \":\")\n",
    "#     context = \" \".join(answers)\n",
    "#     # if (len(answers)>5) context = answers[-5:].join(\" \")\n",
    "#     score_test.append(score_translate_comet_test(\n",
    "#         message = x['question'],\n",
    "#         answer = x['answer'],\n",
    "#         turns = 1,\n",
    "#         history = history_recorder,\n",
    "#         context = context\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "218a922c-9390-46d2-a864-72bc9596847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round(numpy.average(score_test), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3140568-a10b-4538-b8be-47a4f60b62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cec2d-980e-48ea-b976-a1fe8ff83fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
